◎ 월요일~ 화요일
    이번주는 sqoop과정을 진행해 보았다. import와 export를 다루어 보았다. 우선 준비된 데이터가 없어서 이진용 인턴이 사용하던 데이터들을 가지고 진행을 하였다. 준비된 데이터는 3G였는데 SK에서 필요한 데이터 수준은 더 많았다. 대략 50G~100G정도의 큰 규모의 데이터를 처리할 때 각각의 옵션을 적용하여 어떻게하면 효율이 좋아지고 더 빨라지는지를 테스트 해 보는 시간을 가졌다. 우선 export 과정은 import보다 손쉽게 성공을 하였다. export란 Hadoop상에 저장되어있는 데이터 즉 HDFS에 저장된 데이터를 RDBS로 가져오는 명령어이다. 현재 mysql, oracle, netezza 총 세개의 RDBMS에서 테스트를 진행하였는데 나와 이진용인턴은 netezza에서 실험을 하게 되었다. 처음에는 netezza에서 한글 칼럼과 데이터베이스명, 테이블 명이 한글이 지원이 되는지 여부에 대해서 조사를 하였는데 문서들을 뒤져봐도 명확하게 한글을 지원한다는 글을 보지 못하여서 짐작으로 UTF-8을 적용시킨다면 한글도 지원이 되지 않을까 라고 생각을 하여서 시도를 해 보았다. 그러나 동작하는 함수를 UTF-8로 바꾸어 보아도 한글이 안되는 것을 보았고, 그 실행 파일 자체를 UTF-8형식이 적용되도록 바꾸어 보기도 하였으나 한글 칼럼과 테이블명, 데이터베이스를 만들려고 하면 안되는 것을 목격할 수 있었다. 그러나 이진용 인턴이 한글로된 데이터와 테이블명이 한글로된 테이블을 넣는데 성공을 하여서 netezza는 한글이 딘다는 결론이 나왔다. export로 netezza에 3G, 96G, 9G, 46G등의 데이터를 넣는 것을 실험하였고 각각의 결과들을 기록하였다. direct option을 사용하였을때에 조금더 빨라지는 현상을 발견하였고, 문서상으로는 netezza는 8개 까지 mapper를 지우너하기 때문에 mapper의 갯수에 따른 효율을 관찰해보니 netezza에서는 export시에는 mapper를 8개 사용하여서 진행을 하면 1개를 지정했을 때 보다 현저히 빠르다는 것을 알게 되었다. 그러나 import는 테이블에 primary키가 지정이 되어 있지 않다면 mapper가 어느것을 보고 split를 해야할지 모르기 때문에 mapper가 한개밖에 동작을 하지 않는다. 여러개의 mapper를 쓰면 에러가 발생하게 된다. 이에 명시적으로 칼럼을 지정해 준다면 이때는 mapper를 여러개 쓸 수가 있다. 그러나 실험해 본 결과 mapper는 총 4개밖에 돌지 않는다는 것을 발견하였다. 총 8개의 mapper가 생성이 되었다가 4개는 죽고, 4개만 살아남아서 각각 분배가 되는 모습을 관찰 할 수 있었다. 


아래는 전 주에 지속된 문제들을 다시 고치려 해보았으나 발생된 에러들이다.(Oozie)
<img width="954" alt="캡처3day_17" src="https://user-images.githubusercontent.com/37497189/56931142-f9c06200-6b19-11e9-91a7-2bac96d55ddb.PNG">



<img width="1057" alt="캡처3day_18" src="https://user-images.githubusercontent.com/37497189/56931173-19578a80-6b1a-11e9-9211-63a0efbdfee0.png">




<img width="274" alt="캡처4day_1" src="https://user-images.githubusercontent.com/37497189/56931204-33916880-6b1a-11e9-9b86-3d13701e8d6c.PNG">





<img width="951" alt="캡처3day_4" src="https://user-images.githubusercontent.com/37497189/56931250-591e7200-6b1a-11e9-81d7-97ed1b324b32.PNG">


◎수요일 ~ 금요일
    * SK_SPARK 교육
        - SPARK란?
            아파치 스파크(Apache Spark)는 오픈 소스 클러스터 컴퓨팅 프레임워크이다. 원래 캘리포니아 대학교 버클리의 AMPLab에서 개발된 스파크의 코드베이스는 나중에 아파치 소프트웨어 재단에 기부되었으며 그 이후로 계속 유지 보수를 해오고 있다. 스파크는 암시적 데이터 병렬성과 장애 허용과 더불어 완전한 클러스터를 프로그래밍하기 위한 인터페이스를 제공한다. 
        
        - SPARK 기초
            SPARK는 JVM위에서 동작한다. 따라서 JDK가 필요로 하다. SPARK는 내부적으로 Scala라는 언어로 만들어 졌다. Scala는 functional 프로그래밍 언어로서 spark의 장점을 극대화 시켰다. 그리고 Java로 프로그래밍을 한다면 몇백줄이지만 파이썬으로는 50줄정도, scala를 이용하면 20~30줄로 작성이 가능하기 때문에 spark작업시에는 scala언어를 숙지하는게 많이 도움이 된다. 그리고 spark 자체는 두가지 방법으로 동작을 시킬 수 있다. 하나는 Scala shell이고 나머지 하나는 Python shell이다. 숙달된 사람은 Scala Shell을 이용하면 더 빨리 할 수 있지만 현재 교육진행 중에는 Python을 이용하여 Lab을 진행하였다. 
            Python Shell을 접속하기 위해선 pyspark라는 명령어를 치면 되고,Scala Shell을 접속하기 위해서 spark-shell 명령어를 치면 된다.
        
        - RDD란?
            RDD는 Spark의 주요 개념중 하나이다. RDD는 Resilient Distributed Dataset의 약자이다. 말그대로 소생가능한, 복원력있는 분산 데이터한 뜻이다. RDD는 Spark에서 가장 기초가 되는 단위이다. 데이터의 흐름이 RDD로 제어가 되는데 하나의 과정이 지날때마다 이 RDD를 어떻게 처리할 것인지가 Spark의 핵심이다. RDD를 만드는 방법은 세가지가 있다.

            1. file이나 file set으로부터

            2. 메모리의 데이터로 부터

            3. 다른 RDD로부터
            
            이다. RDD는 크게 Transformation 과정과 Action 과정으로 나위어진다. RDD의 결과값은 진행을 하는도안 계속 저장되어진다. 이 때문에 복구가 가능하다는 말이 나오는 것이다. Transformation과정을 거쳐도 이전의 RDD정보는 계속 남겨져 있어서 이전으 RDD도 불러오는 것이 가능하다. 그리고 모든 계산은 action의 과정을 거쳐야지 비로소 진행이 된다. 그 이전까지는 그 계산을 수집만 해놓고 사용자가 필요에 의해서 action의 과정을 진행하면 그때서야 계산을 진행하여 사용자에게 결과값을 전달한다. 이때문에 처리 방식은 Lazy Execution다. 이는 고 가용성과 효율을 위해 이런 구조를 가지게 된 것이다. 만약 spark가 이런 lazy방식이 아닌 실시간으로 데이터를 처리하려고 한다면 속도가 엄청나게 느려질 것이다. 하나의 transformation과정을 진행하고 그 값을을 계산한뒤에 결과값을 리턴 시켜주고 이러다보면 방대한 데이터들을 처리하는데 엄청난 시간이 들기 마련이다. 그리고 RDD는 imutable한 데이터이다. 이는 무슨 뜻이냐면 데이터가 불변이라는 뜻이다. 필요에 의해서 새로운 RDD로 바꿀수는 있지만 기존의 RDD를 바꾸지는 못한다. 
            action과정 중 기본은 count(), collect(), take(), saveAsTextFile() 등이 있다. 이들 모두 값을 얻거나 저장을 하는데 사용하는 함수들이다. Spark에서 기본 프로그래밍 약속중 하나는 함수를 사용할때 saveAsFile처럼 처음오는 단어는 소문자, 그 다음 오는 단어는 대문자로 쓰는것을 기본으로 하고있다. 
            그 다음 transformation의 기본적인 함수들은 map과 filter이다. 거의 대부분의 transformation 과정은 이 두 함수를 이용한다고 보면 된다. 실제 lab을 돌려본 결과 RDD를 다루는데 거의 대부분이 map함수로 transformation을 하는 과정이었다. 이 과정들은 모두 chaining을 하여서 진행을 할 수 있다. spark는 scala기반이기 때문에 functional하다는 말은 앞서 했었는데 map과정을 .map()이런 식으로 계속 chaining을 하여 이어가는 것이 가능하다. java나 다른 언어에서는 보기 힘든 구조여서 흥미가 생겼다. 이 모든과정이 팡프라인을 형성하여서 진행이 된다. spark lab과정 맨처음은 데이터를 가져오는 것 부터 시작을 한다. 데이터를 가져오는 방식은 sc.textFile("파일이름")을 이용한다. 그 다음 text파일을 읽어들여서 RDD로 변환이 된다. 이때 RDD element의 구분은 줄 단위로 진행이 된다.

        - YARN과의 비교
            SPARK는 결국 MapReduce 처리를 한다는 점에서는 YARN과 비슷하다. SPARK는 독자적으로 동작을 할 수 있지만 YARN과 같이 CM에 올려서 동작을 시킬 수도 있다.
        
        - client 모드와 cluster모드
            client모드는 말 그대로 client단에서 실행잉 되는 것을 말한다. 이는 간단히 테스트 용으로는 괜찮지만 프로젝트나 실제로 쓰일때는 client에게 모든 정보들을 보여줄 수 없기때문에 cluster모드를 사용하는 것을 추천한다. cluster모드는 custer 내부에서 동작을 하기 때문에 좀더 빠르고 보안적인 측면도 높일 수 있다.


        - Lab 실습과정
            실습과정중 모르거나 아레된 사실들
                spark를 돌릴때 shell을켜서 하는방법만 앞서 설명을 하였는데
                실제로는 한가지 작성 방법이 더 있다. Jupyter 노트북으로 작성하는 방법이다. 여기서는 더 간단하게 결과값들을 볼 수 있고 실시간 으로 데이터가 핸들링 되는것을 볼 수가 있었다. 그러나 Jupyter 노트북을 가동한 상태에서는 안먹히는 명령어들도 있었기 때문에 실습간에 그 점에 유의를 하고 진행을 했어야 했다. 실제로도 교육을 듣는 대다수의 분들이 이 부분에서 막혔었고 질문을 많이 하셨다.

        - Spark 동작 확인
            Spark가 제대로 동작하고 있는지, 진행상황은 어떤지를 확인하기 위해서는 Spark Web UI를 확인하는 방법과 YARN UI에서 job들이 동작하는 것들을 보고 알 수 있다. 그리고 RDD들이 stage에 의해서 바뀌는 것을 Spark web UI를 통해 확인을 할 수 있다.